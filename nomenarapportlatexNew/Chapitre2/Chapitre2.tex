%------------------------------début entêtes-----------------------------------------
\pagestyle{fancy}

\renewcommand{\footrulewidth}{1pt}

\fancyhead[L]{\footnotesize \rightmark}
\fancyhead[C]{\thepage}
\fancyhead[R]{Discussion}

\fancyfoot[L]{Ny Hoavy Nomena}
\fancyfoot[C]{\thepage}
\fancyfoot[R]{Annotation automatique d'images}

%------------------------------fin entêtes-------------------------------------------

\chapter{Discussions} \label{discussion}

D'après les résultats obtenus, l'ajout d'informations sémantiques sur le contenu de l'image améliore la performance du modèle de base m-RNN à générer les phrases descriptives. Ces informations, sous forme de vecteurs de catégories, peuvent être introduites dans le modèle m-RNN par projection sur l'espace multimodal. 
%Dans notre travail, ces informations ont été représentés par le vecteur de catégories associé à l'image.
Une comparaison entre les phrases générées par notre modèle  et celles de la collection de données est effectuée pour une évaluation concrète des résultats.

Dans les exemples de descriptions de la table \ref{tab:resdesc}, nous pouvons relever des erreurs qui causent la non-correspondance entre les phrases de la collection des données et les phrases générées par notre modèle. En général, les erreurs concernent la non-identification de certains concepts présents dans l'image. Ces erreurs ont été induites par la performance de la partie visuelle du modèle: le descripteur et notre modèle de classification. 

%--futur work: 
Nous pouvons encore explorer plusieurs idées pour améliorer notre modèle, à savoir: \\
- une expérimentation sur les hyperparamètres: nous pourrions varier la configuration de notre modèle notamment le nombre de couches du RNN et la dimension de l'espace de projection multimodale pour obtenir un modèle plus performant,\\
- une amélioration du modèle de classification: en faisant un traitement sur les régions pertinentes de l'image pour atteindre la performance de la version optimale.

 


\chapter{Conclusion et perspective}
%en cours
 Le but de notre étude est d'explorer les modèles de l'apprentissage profond pour l'annotation automatique des images. Ainsi, l'analyse des images et textes de la collection de données nous a amené aux travaux issus de la vision par ordinateur et le traitement automatique du langage naturel. 

D'une part, les travaux en vision par ordinateur concernent l'extraction de vecteurs caractéristiques des images numériques à partir des réseaux de neurones convolutifs et la classification de ces images. 


D'une autre part, les travaux en traitement automatique du langage naturel nous ont permis de modéliser la génération de séquences de mots en utilisant les réseaux de neurones récurrents.

En associant ces travaux nous avons défini un modèle qui permet de générer automatiquement des phrases décrivant les images .

Nous avons apporté une amélioration du modèle de base multimodal recurrent neural networks: m-RNN \cite{mao2014explain} \cite{mao2014deep} pour la description des images de la collection de données de Microsoft COCO Caption \cite{chen2015microsoft} en utilisant les informations sur les catégories auxquelles les images appartiennent sous forme de vecteurs de catégories.

Les résultats confirment l'amélioration effectuée sur ce modèle de base sur la performance des modèles par rapport aux mesures BLEU, METEOR et CIDEr.

Ces résultats peuvent encore être améliorés grâce aux travaux futurs proposés dans la discussion pour atteindre la performance du modèle optimal.

%--futurs projets:
Concernant les futurs projets pour l'annotation automatique des images, nous pouvons entamer une annotation des images par génération de descriptions sur des régions de l'image. Cela permet de fournir des informations plus précises et détaillées sur le contenu de l'image en faisant un traitement sur chaque région pertinente de l'image. Visual genome \cite{krishna2016visual}, utilisée dans \cite{johnson2015densecap}, est une collection de données qui permet d'expérimenter sur ce problème.

